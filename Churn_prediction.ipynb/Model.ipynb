{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_preprocessing\n",
    "\n",
    "def data_preprocessing(dataset:pd.DataFrame):\n",
    "    '''\n",
    "    dataset contain this columns:\n",
    "        'gender':               ['Female' 'Male']\n",
    "        'SeniorCitizen':        [0 1]\n",
    "        'Partner':              ['Yes' 'No']\n",
    "        'Dependents':           ['No' 'Yes']\n",
    "        'tenure':               int\n",
    "        'PhoneService':         ['No' 'Yes']\n",
    "        'MultipleLines':        ['No phone service' 'No' 'Yes']\n",
    "        'InternetService':      ['DSL' 'Fiber optic' 'No']\n",
    "        'OnlineSecurity':       ['No' 'Yes' 'No internet service']\n",
    "        'OnlineBackup':         ['Yes' 'No' 'No internet service']\n",
    "        'DeviceProtection':     ['No' 'Yes' 'No internet service']\n",
    "        'TechSupport':          ['No' 'Yes' 'No internet service']\n",
    "        'StreamingTV':          ['No' 'Yes' 'No internet service']\n",
    "        'StreamingMovies':      ['No' 'Yes' 'No internet service']\n",
    "        'Contract':             ['Month-to-month' 'One year' 'Two year']\n",
    "        'PaperlessBilling':     ['Yes' 'No']\n",
    "        'PaymentMethod':        ['Electronic check' 'Mailed check' 'Bank transfer (automatic)' 'Credit card (automatic)']\n",
    "        'MonthlyCharges':       float\n",
    "        'TotalCharges':         float\n",
    "        'Churn':                ['No' 'Yes']   // Not necessary\n",
    "    '''\n",
    "    df = dataset.copy()\n",
    "\n",
    "    # Prepare string variables\n",
    "    df.columns = df.columns.str.lower().str.replace(' ','_')\n",
    "    string_columns = list(df.dtypes[df.dtypes=='O'].index)\n",
    "    for col in string_columns:\n",
    "        df[col] = df[col].str.lower().str.replace(' ','_')\n",
    "\n",
    "    # drop customerid\n",
    "    if 'customerid' in df.columns:\n",
    "        del df['customerid']\n",
    "        \n",
    "    # Prepare numeric columns type\n",
    "    if 'tenure' in df.columns:\n",
    "        df['tenure'] = pd.to_numeric(df['tenure'], errors='coerce')\n",
    "        df['tenure'] = df['tenure'].fillna(0)\n",
    "    else:\n",
    "        raise Exception(\"!!! Tenure column didn't exist !!!\")\n",
    "\n",
    "    if 'monthlycharges' in df.columns:\n",
    "        df['monthlycharges'] = pd.to_numeric(df['monthlycharges'], errors='coerce')\n",
    "        df['monthlycharges'] = df['monthlycharges'].fillna(0)\n",
    "    else:\n",
    "        raise Exception(\"!!! MonthlyCharges column didn't exist !!!\")\n",
    "\n",
    "    if 'totalcharges' in df.columns:\n",
    "        df['totalcharges'] = pd.to_numeric(df['totalcharges'], errors='coerce')\n",
    "        df['totalcharges'] = df['totalcharges'].fillna(0)\n",
    "    else:\n",
    "        raise Exception(\"!!! TotalCharges column didn't exist !!!\")\n",
    "        \n",
    "    # Make dependent variable numeric\n",
    "    if 'churn' in df.columns:\n",
    "        df.churn = (df.churn == 'yes').astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeatureEngineering\n",
    "\n",
    "def FeatureEngineering(dataset:pd.DataFrame, categorical_variables:list, numerical_variables:list, isTrain:bool):\n",
    "    \"\"\"\n",
    "    dataset: pd.Dataframe\n",
    "    categorical_variables: list of important categorical columns that we want to use in pur model\n",
    "    numerical_variables: list of important numerical columns that we want to use in pur model\n",
    "    isTrain:    if you want to train a model, set it 'True'\n",
    "                if you want to use for test or deployment set it 'False'\n",
    "    \"\"\"\n",
    "    df = dataset.copy()\n",
    "    categorical_important = categorical_variables.copy()\n",
    "    numerical_important = numerical_variables.copy()\n",
    "    \n",
    "    df = df[numerical_important + categorical_important]\n",
    "    \n",
    "    # Dummy variables\n",
    "    if isTrain:\n",
    "        OHE = make_column_transformer((OneHotEncoder(), categorical_important ),\n",
    "                                            remainder='passthrough',\n",
    "                                            verbose_feature_names_out=False)\n",
    "        ohe = OHE.fit_transform(df)\n",
    "        pickle.dump(ohe, open('OneHotEncoder.pkl','wb'))\n",
    "        df = pd.DataFrame(ohe, columns=OHE.get_feature_names_out())\n",
    "\n",
    "        # we Save OHE.get_feature_names_out() to use in test process or deployment in future\n",
    "        with open(\"OHE_feature_names_out.txt\", \"w\") as file:\n",
    "            for item in OHE.get_feature_names_out():\n",
    "                file.write('%s\\n' %item)\n",
    "        file.close()\n",
    "    else:\n",
    "        ohe = pickle.load(open('OneHotEncoder.pkl','rb'))\n",
    "        OHE_feature_names_out=[]\n",
    "        with open('OHE_feature_names_out.txt') as file:\n",
    "            OHE_feature_names_out = file.readlines()\n",
    "        file.close()\n",
    "        OHE_feature_names_out = list(map(lambda x: x[:-1], OHE_feature_names_out))\n",
    "\n",
    "        df = pd.DataFrame(ohe, columns=OHE_feature_names_out)\n",
    "        \n",
    "    # Feature scaling\n",
    "    if isTrain:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df)\n",
    "        df = scaler.transform(df)\n",
    "        df = pd.DataFrame(df, columns=scaler.feature_names_in_)\n",
    "        pickle.dump(scaler, open('scaler.pkl','wb'))\n",
    "    else:\n",
    "        scaler = pickle.load(open('scaler.pkl','rb'))\n",
    "        df = scaler.transform(df)\n",
    "        df = pd.DataFrame(df, columns=scaler.feature_names_in_)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Churn_prediction.csv')\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Split dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.loc[:,df.columns!='Churn'],\n",
    "                                                    df['Churn'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=666)\n",
    "\n",
    "X_train.reset_index(inplace=True,drop=True)\n",
    "X_test.reset_index(inplace=True,drop=True)\n",
    "\n",
    "y_train = [1 if x == 'Yes' else 0 for x in y_train]\n",
    "y_test = [1 if x == 'Yes' else 0 for x in y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length X_train:5634, lengthy_train:5634\n",
      "Length X_test:1409, lengthy_test:1409\n"
     ]
    }
   ],
   "source": [
    "print(\"Length X_train:{0}, lengthy_train:{1}\".format(len(X_train),len(y_train)))\n",
    "print(\"Length X_test:{0}, lengthy_test:{1}\".format(len(X_test),len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_important = ['contract', 'onlinesecurity', 'techsupport', 'internetservice']\n",
    "numerical_important = ['tenure', 'monthlycharges', 'totalcharges']\n",
    "\n",
    "X_train = data_preprocessing(X_train)\n",
    "X_train = FeatureEngineering(X_train,\n",
    "                              categorical_variables=categorical_important,\n",
    "                              numerical_variables=numerical_important,\n",
    "                              isTrain=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1) Logistic Regression</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8367262420328858\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_LogisticRegression = LogisticRegression(random_state=0)\n",
    "\n",
    "scores = cross_validate(clf_LogisticRegression, X_train, y_train, cv=6,\n",
    "                        scoring=('roc_auc'),\n",
    "                        return_train_score=True)\n",
    "print(scores['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2) K Nearest Neighbors</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7796812937086126\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_KNN = KNeighborsClassifier()\n",
    "\n",
    "scores = cross_validate(clf_KNN, X_train, y_train, cv=6,\n",
    "                        scoring=('roc_auc'),\n",
    "                        return_train_score=True)\n",
    "print(scores['test_score'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3) Naive Bayes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8254343639998684\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf_NB = GaussianNB()\n",
    "\n",
    "scores = cross_validate(clf_NB, X_train, y_train, cv=6,\n",
    "                        scoring=('roc_auc'),\n",
    "                        return_train_score=True)\n",
    "print(scores['test_score'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4) Support Vector machine for Classification (SVC)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8092300188309788\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf_SVC = SVC(kernel='linear', random_state=6)\n",
    "\n",
    "scores = cross_validate(clf_SVC, X_train, y_train, cv=6,\n",
    "                        scoring=('roc_auc'),\n",
    "                        return_train_score=True)\n",
    "print(scores['test_score'].mean())\n",
    "# kernel\n",
    "#  -sigmoid   0.7217249049598338\n",
    "#  -poly      0.8158020900887415\n",
    "#  -rbf       0.7720218063211098\n",
    "#  -linear    0.8092300188309788\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>5) Desicion Tree Classification</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6673704556237601\n"
     ]
    }
   ],
   "source": [
    "from calendar import c\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_DT = DecisionTreeClassifier(criterion='entropy', random_state=6)\n",
    "\n",
    "scores = cross_validate(clf_DT, X_train, y_train, cv=6,\n",
    "                        scoring=('roc_auc'),\n",
    "                        return_train_score=True)\n",
    "print(scores['test_score'].mean())\n",
    "# criterion\n",
    "#   -gini      0.666598842172868\n",
    "#   -entropy   0.6673704556237601"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>6) Random Forest</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7955202892868366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_RF = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=6)\n",
    "\n",
    "scores = cross_validate(clf_RF, X_train, y_train, cv=6,\n",
    "                        scoring=('roc_auc'),\n",
    "                        return_train_score=True)\n",
    "print(scores['test_score'].mean())\n",
    "# criterion\n",
    "#   -gini       0.7940777923624925    \n",
    "#   -entropy    0.7955202892868366  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>7) XGBoost</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8171957479568427\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf_xgboost = XGBClassifier()\n",
    "\n",
    "scores = cross_validate(clf_xgboost, X_train, y_train, cv=6,\n",
    "                        scoring=('roc_auc'),\n",
    "                        return_train_score=True)\n",
    "print(scores['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>8) Gradiant Boosting model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8405381208558101\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf_GB = GradientBoostingClassifier(n_estimators=20, learning_rate=0.1, max_depth=3)\n",
    "\n",
    "scores = cross_validate(clf_GB, X_train, y_train, cv=6,\n",
    "                        scoring=('roc_auc'),\n",
    "                        return_train_score=True)\n",
    "print(scores['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TEST</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length X_train:5634, lengthy_train:5634\n",
      "Length X_test:1409, lengthy_test:1409\n"
     ]
    }
   ],
   "source": [
    "print(\"Length X_train:{0}, lengthy_train:{1}\".format(len(X_train),len(y_train)))\n",
    "print(\"Length X_test:{0}, lengthy_test:{1}\".format(len(X_test),len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Length X_train:5634, lengthy_train:5634\n",
      "Length X_test:1409, lengthy_test:1409\n",
      "2\n",
      "Length X_train:5634, lengthy_train:5634\n",
      "Length X_test:5634, lengthy_test:1409\n",
      "3\n",
      "Length X_train:5634, lengthy_train:5634\n",
      "Length X_test:5634, lengthy_test:1409\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1409, 5634]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Github\\Jupyters\\Jupyter_notebooks\\Churn_prediction.ipynb\\Model.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Github/Jupyters/Jupyter_notebooks/Churn_prediction.ipynb/Model.ipynb#X31sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m pred \u001b[39m=\u001b[39m clf_LogisticRegression\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Github/Jupyters/Jupyter_notebooks/Churn_prediction.ipynb/Model.ipynb#X31sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m roc_auc_score\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Github/Jupyters/Jupyter_notebooks/Churn_prediction.ipynb/Model.ipynb#X31sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m roc_auc_score(y_test, pred)\n",
      "File \u001b[1;32mc:\\Users\\Amin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:571\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    569\u001b[0m     labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y_true)\n\u001b[0;32m    570\u001b[0m     y_true \u001b[39m=\u001b[39m label_binarize(y_true, classes\u001b[39m=\u001b[39mlabels)[:, \u001b[39m0\u001b[39m]\n\u001b[1;32m--> 571\u001b[0m     \u001b[39mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    572\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[39m=\u001b[39;49mmax_fpr),\n\u001b[0;32m    573\u001b[0m         y_true,\n\u001b[0;32m    574\u001b[0m         y_score,\n\u001b[0;32m    575\u001b[0m         average,\n\u001b[0;32m    576\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    577\u001b[0m     )\n\u001b[0;32m    578\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# multilabel-indicator\u001b[39;00m\n\u001b[0;32m    579\u001b[0m     \u001b[39mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    580\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[39m=\u001b[39mmax_fpr),\n\u001b[0;32m    581\u001b[0m         y_true,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    584\u001b[0m         sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[0;32m    585\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Amin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_base.py:75\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m format is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m binary_metric(y_true, y_score, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m     77\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m     78\u001b[0m y_true \u001b[39m=\u001b[39m check_array(y_true)\n",
      "File \u001b[1;32mc:\\Users\\Amin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:344\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[1;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(y_true)) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    340\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    341\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mis not defined in that case.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    342\u001b[0m     )\n\u001b[1;32m--> 344\u001b[0m fpr, tpr, _ \u001b[39m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n\u001b[0;32m    345\u001b[0m \u001b[39mif\u001b[39;00m max_fpr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m max_fpr \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    346\u001b[0m     \u001b[39mreturn\u001b[39;00m auc(fpr, tpr)\n",
      "File \u001b[1;32mc:\\Users\\Amin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:981\u001b[0m, in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mroc_curve\u001b[39m(\n\u001b[0;32m    893\u001b[0m     y_true, y_score, \u001b[39m*\u001b[39m, pos_label\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, drop_intermediate\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    894\u001b[0m ):\n\u001b[0;32m    895\u001b[0m     \u001b[39m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \n\u001b[0;32m    897\u001b[0m \u001b[39m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    979\u001b[0m \n\u001b[0;32m    980\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m     fps, tps, thresholds \u001b[39m=\u001b[39m _binary_clf_curve(\n\u001b[0;32m    982\u001b[0m         y_true, y_score, pos_label\u001b[39m=\u001b[39;49mpos_label, sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[0;32m    983\u001b[0m     )\n\u001b[0;32m    985\u001b[0m     \u001b[39m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[0;32m    986\u001b[0m     \u001b[39m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[0;32m    987\u001b[0m     \u001b[39m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[39m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[0;32m    993\u001b[0m     \u001b[39m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[0;32m    994\u001b[0m     \u001b[39mif\u001b[39;00m drop_intermediate \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(fps) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Amin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:742\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m (y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m pos_label \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)):\n\u001b[0;32m    740\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m format is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[1;32m--> 742\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m    743\u001b[0m y_true \u001b[39m=\u001b[39m column_or_1d(y_true)\n\u001b[0;32m    744\u001b[0m y_score \u001b[39m=\u001b[39m column_or_1d(y_score)\n",
      "File \u001b[1;32mc:\\Users\\Amin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:387\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    385\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    386\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 387\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    390\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1409, 5634]"
     ]
    }
   ],
   "source": [
    "X_test = data_preprocessing(X_test)\n",
    "print(1)\n",
    "print(\"Length X_train:{0}, lengthy_train:{1}\".format(len(X_train),len(y_train)))\n",
    "print(\"Length X_test:{0}, lengthy_test:{1}\".format(len(X_test),len(y_test)))\n",
    "X_test = FeatureEngineering(X_test,\n",
    "                            categorical_variables=categorical_important,\n",
    "                            numerical_variables=numerical_important,\n",
    "                            isTrain=False)\n",
    "print(2)\n",
    "print(\"Length X_train:{0}, lengthy_train:{1}\".format(len(X_train),len(y_train)))\n",
    "print(\"Length X_test:{0}, lengthy_test:{1}\".format(len(X_test),len(y_test)))\n",
    "# model\n",
    "clf_LogisticRegression = LogisticRegression(random_state=0)\n",
    "clf_LogisticRegression.fit(X_train, y_train)\n",
    "print(3)\n",
    "print(\"Length X_train:{0}, lengthy_train:{1}\".format(len(X_train),len(y_train)))\n",
    "print(\"Length X_test:{0}, lengthy_test:{1}\".format(len(X_test),len(y_test)))\n",
    "# clf_GB = GradientBoostingClassifier(n_estimators=20, learning_rate=0.1, max_depth=3)\n",
    "# clf_GB.fit(df_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "pred = clf_LogisticRegression.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, pred)\n",
    "# 0.6610603450428183"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aafc33a19d1e6c143b5a4b283b91d6d8d620b2aadbd9675105175468fd18e65b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
